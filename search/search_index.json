{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello \ud83d\udc4b","text":"<p>My name is Gary Doing fun things as @ospf2fullstack</p> <p>     - </p> Quote <pre><code>\"Everything fails, all the time\" ~ Dr. Werner Vogels\n</code></pre>"},{"location":"#virtual-meet-and-greet","title":"Virtual Meet and Greet","text":"<p>LinkedIn</p> <ul> <li>\ud83d\udd2d I\u2019m currently working on Digital Twins. </li> <li>\ud83c\udf31 Learning Terraform, Kubernetes and Javascript and all things DevOps.</li> </ul>"},{"location":"#things-i-break","title":"Things I Break","text":"<ul> <li>Kubernetes </li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li>Intel NUK</li> <li>Dell Servers</li> <li>Blades (coming soon)</li> </ul>"},{"location":"100_days/100days_devops/","title":"100days devops","text":"<ul> <li>100 Days of DevOps</li> <li>Origination</li> </ul>"},{"location":"100_days/100days_devops/#100-days-of-devops","title":"100 Days of DevOps","text":""},{"location":"100_days/100days_devops/#origination","title":"Origination","text":"<p>You can access a forked version here</p>"},{"location":"100_days/Day_1_CloudWatch/","title":"Day 1 CloudWatch","text":"<ul> <li>Day 1</li> <li>CloudWatch Metrics</li> </ul>"},{"location":"100_days/Day_1_CloudWatch/#day-1","title":"Day 1","text":""},{"location":"100_days/Day_1_CloudWatch/#cloudwatch-metrics","title":"CloudWatch Metrics","text":"<p>Relates to: [[100days_devops]]</p>"},{"location":"aws/AWS-IoT/","title":"AWS-IoT","text":"<p>Info</p> <p>Content coming soon. Still migrating documentation. </p> <p>Configure IoT MQTT on AWS.1</p> <ol> <li> <p>These are my notes, not intended to be taken as \"truth\".\u00a0\u21a9</p> </li> </ol>"},{"location":"aws/eks/","title":"Eks","text":"<p>Create an AWS EKS <code>elastic kubernetes service</code> cluster</p> <ul> <li>Elastic Kubernetes Service (GUI)</li> <li>EKS IAM Role</li> <li>Trusted Entity</li> <li>EKS Cluster Policy</li> <li>Cluster Configuration</li> </ul>"},{"location":"aws/eks/#elastic-kubernetes-service-gui","title":"Elastic Kubernetes Service (GUI)","text":"<p>Requirements   - IAM Cluster Role for EKS   - Configure Cluster in EKS page</p>"},{"location":"aws/eks/#eks-iam-role","title":"EKS IAM Role","text":""},{"location":"aws/eks/#trusted-entity","title":"Trusted Entity","text":"<p><code>{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Principal\": {                 \"Service\": [                     \"eks.amazonaws.com\"                 ]             },             \"Action\": \"sts:AssumeRole\"         }     ] }</code></p>"},{"location":"aws/eks/#eks-cluster-policy","title":"EKS Cluster Policy","text":"<p><code>{     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"autoscaling:DescribeAutoScalingGroups\",                 \"autoscaling:UpdateAutoScalingGroup\",                 \"ec2:AttachVolume\",                 \"ec2:AuthorizeSecurityGroupIngress\",                 \"ec2:CreateRoute\",                 \"ec2:CreateSecurityGroup\",                 \"ec2:CreateTags\",                 \"ec2:CreateVolume\",                 \"ec2:DeleteRoute\",                 \"ec2:DeleteSecurityGroup\",                 \"ec2:DeleteVolume\",                 \"ec2:DescribeInstances\",                 \"ec2:DescribeRouteTables\",                 \"ec2:DescribeSecurityGroups\",                 \"ec2:DescribeSubnets\",                 \"ec2:DescribeVolumes\",                 \"ec2:DescribeVolumesModifications\",                 \"ec2:DescribeVpcs\",                 \"ec2:DescribeDhcpOptions\",                 \"ec2:DescribeNetworkInterfaces\",                 \"ec2:DescribeAvailabilityZones\",                 \"ec2:DetachVolume\",                 \"ec2:ModifyInstanceAttribute\",                 \"ec2:ModifyVolume\",                 \"ec2:RevokeSecurityGroupIngress\",                 \"ec2:DescribeAccountAttributes\",                 \"ec2:DescribeAddresses\",                 \"ec2:DescribeInternetGateways\",                 \"elasticloadbalancing:AddTags\",                 \"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\",                 \"elasticloadbalancing:AttachLoadBalancerToSubnets\",                 \"elasticloadbalancing:ConfigureHealthCheck\",                 \"elasticloadbalancing:CreateListener\",                 \"elasticloadbalancing:CreateLoadBalancer\",                 \"elasticloadbalancing:CreateLoadBalancerListeners\",                 \"elasticloadbalancing:CreateLoadBalancerPolicy\",                 \"elasticloadbalancing:CreateTargetGroup\",                 \"elasticloadbalancing:DeleteListener\",                 \"elasticloadbalancing:DeleteLoadBalancer\",                 \"elasticloadbalancing:DeleteLoadBalancerListeners\",                 \"elasticloadbalancing:DeleteTargetGroup\",                 \"elasticloadbalancing:DeregisterInstancesFromLoadBalancer\",                 \"elasticloadbalancing:DeregisterTargets\",                 \"elasticloadbalancing:DescribeListeners\",                 \"elasticloadbalancing:DescribeLoadBalancerAttributes\",                 \"elasticloadbalancing:DescribeLoadBalancerPolicies\",                 \"elasticloadbalancing:DescribeLoadBalancers\",                 \"elasticloadbalancing:DescribeTargetGroupAttributes\",                 \"elasticloadbalancing:DescribeTargetGroups\",                 \"elasticloadbalancing:DescribeTargetHealth\",                 \"elasticloadbalancing:DetachLoadBalancerFromSubnets\",                 \"elasticloadbalancing:ModifyListener\",                 \"elasticloadbalancing:ModifyLoadBalancerAttributes\",                 \"elasticloadbalancing:ModifyTargetGroup\",                 \"elasticloadbalancing:ModifyTargetGroupAttributes\",                 \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",                 \"elasticloadbalancing:RegisterTargets\",                 \"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\",                 \"elasticloadbalancing:SetLoadBalancerPoliciesOfListener\",                 \"kms:DescribeKey\"             ],             \"Resource\": \"*\"         },         {             \"Effect\": \"Allow\",             \"Action\": \"iam:CreateServiceLinkedRole\",             \"Resource\": \"*\",             \"Condition\": {                 \"StringEquals\": {                     \"iam:AWSServiceName\": \"elasticloadbalancing.amazonaws.com\"                 }             }         }     ] }</code></p>"},{"location":"aws/eks/#cluster-configuration","title":"Cluster Configuration","text":"<p>Select the role from the previous step</p>"},{"location":"cloud_resume/cloud_resume_challenge/","title":"Cloud resume challenge","text":"<pre><code>sequenceDiagram\n    autonumber\n    Cloud Resume Challenge -&gt;&gt; Static Site : Code\n</code></pre>"},{"location":"cloud_resume/cloud_resume_challenge/#cloud-resume-challenge-for-aws","title":"Cloud Resume Challenge for AWS","text":"<p>Original Challenge Link</p> <p>My Completed Project</p>"},{"location":"cloud_resume/cloud_resume_challenge/#certification","title":"Certification","text":""},{"location":"cloud_resume/cloud_resume_challenge/#pre-req","title":"Pre-req","text":"<p> Certification Your resume needs to have the AWS Cloud Practitioner certification on it. This is an introductory certification that orients you on the industry-leading AWS cloud.</p> <p></p>"},{"location":"cloud_resume/cloud_resume_challenge/#the-html","title":"The HTML","text":"<p> HTML Your resume needs to be written in HTML. Not a Word doc, not a PDF. Here is an example of what I mean.</p>"},{"location":"cloud_resume/cloud_resume_challenge/#html-code","title":"HTML Code","text":"<pre><code>&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;My Name, &lt;/h1&gt;\n        &lt;p&gt;Some beautiful information about me. &lt;/p&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"cloud_resume/cloud_resume_challenge/#the-css","title":"The CSS","text":"<p> CSS Your resume needs to be styled with CSS. No worries if you\u2019re not a designer \u2013 neither am I. It doesn\u2019t have to be fancy. But we need to see something other than raw HTML when we open the webpage.</p>"},{"location":"cloud_resume/cloud_resume_challenge/#css-code","title":"CSS Code","text":"<pre><code>body {\nfont-family: \"Georgia\";\nbackground: #2980b9;\noverflow: hidden;\nheight: 100%;\nwidth: 100%;\n}\n\nh1 {\ncolor: white;\nfont-size: 2em;\nfont-weight: 100;\nletter-spacing: 0.2em;\nposition: absolute;\ntop: 40%;\nleft: 50%;\ntransform: translate3d(-50%, -50%, 0);\n}\n\np {\ncolor: white;\nfont-size: 1em;\nfont-weight: 100;\nletter-spacing: 0.2em;\nposition: absolute;\ntop: 55%;\nleft: 50%;\ntransform: translate3d(-50%, -50%, 0);\n}\n</code></pre>"},{"location":"cloud_resume/cloud_resume_challenge/#static-website","title":"Static Website","text":"<p> Static Website Your HTML resume should be deployed online as an Amazon S3 static website. Services like Netlify and GitHub Pages are great and I would normally recommend them for personal static site deployments, but they make things a little too abstract for our purposes here. Use S3.</p> <p>Make it harder! Deploy s3 with terraform </p>"},{"location":"cloud_resume/cloud_resume_challenge/#terraform","title":"Terraform","text":"<pre><code># create le bucket\nresource \"aws_s3_bucket\" \"awscloudresumechallenge\" {\nbucket = \"awscloudresumechallenge.garyinnerarity.com\"\n}\n\n# create static website configuration (attahc to bucket)\nresource \"aws_s3_bucket_website_configuration\" \"awscloudresumechallengeconfig\" {\nbucket = var.awscloudresumechallenge\n\nindex_document {\nsuffix = \"index.html\"\n}\n\n}\n\n# attach policy to s3 bucket that is le static website resource \"aws_s3_bucket_public_access_block\" \"innerarity-allow-public\" {\nbucket = var.awscloudresumechallenge\n\nblock_public_acls = false\nblock_public_policy = false\nignore_public_acls = false\nrestrict_public_buckets = false\n}\n</code></pre>"},{"location":"cloud_resume/cloud_resume_challenge/#pipelines-the-dirty-type","title":"Pipelines (the dirty type  )","text":"<pre><code>name: SyncHub2S3\n\non:\npush:\nbranches: [ main ]\nworkflow_dispatch:\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n# This workflow contains a single job called \"build\"\nbuild:\n# The type of runner that the job will run on\nruns-on: ubuntu-latest\n# https://github.com/marketplace/actions/s3-sync \nsteps:\n- uses: actions/checkout@master\n- uses: jakejarvis/s3-sync-action@master\nwith:\nargs: --acl public-read --follow-symlinks --delete --exclude '.git/*' --exclude '.github/*'\nenv:\nAWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}\nAWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\nAWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\nAWS_REGION: ${{ secrets.AWS_REGION }}\nSOURCE_DIR: '.'\n</code></pre> <p>If you have the AWS account, use the <code>a</code> record to create an alias to the s3 service. </p>"},{"location":"cloud_resume/cloud_resume_challenge/#https","title":"HTTPS","text":"<p> HTTPS The S3 website URL should use HTTPS for security. You will need to use Amazon CloudFront to help with this.</p> <p><code>AWS Certificate Manager is your friend.</code></p>"},{"location":"cloud_resume/cloud_resume_challenge/#cname","title":"CNAME","text":"<p> DNS Point a custom DNS domain name to the CloudFront distribution, so your resume can be accessed at something like my-c00l-resume-website.com. You can use Amazon Route 53 or any other DNS provider for this. A domain name usually costs about ten bucks to register.</p> <p>This part is VERY easy if you have AWS Route53 as your DNS provider and registrar - which I do, but I also use CloudFlare to make my life a pain in the ass... So, at the moment, I'm using CloudFlare as my registrar. </p> <p>So, point the sub-domain to the s3 bucket address with a CNAME record:  <pre><code>CNAME awscloudresumechallenge -&gt; awscloudresumechallenge.garyinnerarity.com.s3-website-us-east-1.amazonaws.com </code></pre></p>"},{"location":"cloud_resume/cloud_resume_challenge/#javascript","title":"JavaScript","text":"<p> Javascript Your resume webpage should include a visitor counter that displays how many people have accessed the site. You will need to write a bit of Javascript to make this happen. Here is a helpful tutorial to get you started in the right direction.</p> <pre><code>// the basics - below in the api section i'll included updated code\nvar counterContainer = document.querySelector(\".visitorcounter\");\nvar visitCount = localStorage.getItem(\"page_view\");\nvisitCount = 100013;\n\n//add entry for key=\"page_view\"\nlocalStorage.setItem(\"page_view\", 1);\n\nvisitCount = Number(visitCount) + 1;\n\n//update local storage value\nlocalStorage.setItem(\"page_view\", visitCount);\n\ncounterContainer.innerHTML = visitCount;\n</code></pre>"},{"location":"cloud_resume/cloud_resume_challenge/#database-dynamodb","title":"Database (dynamodb)","text":"<p> Database The visitor counter will need to retrieve and update its count in a database somewhere. I suggest you use Amazon\u2019s DynamoDB for this. (Use on-demand pricing for the database and you\u2019ll pay essentially nothing, unless you store or retrieve much more data than this project requires.) Here is a great free course on DynamoDB.</p> <pre><code>// dynamodb configuration\n{\n\"VisitorID\": {\n\"S\": \"\"\n},\n\"CreationDate\": {\n\"S\": \"\"\n}\n}\n</code></pre>"},{"location":"cloud_resume/cloud_resume_challenge/#api","title":"API","text":"<p> API Do not communicate directly with DynamoDB from your Javascript code. Instead, you will need to create an API that accepts requests from your web app and communicates with the database. I suggest using AWS\u2019s API Gateway and Lambda services for this. They will be free or close to free for what we are doing.</p> <p>Look, I followed this okay., don't forget to create your IAM role before making your REST API. </p> <p>[ ] Python You will need to write a bit of code in the Lambda function; you could use more Javascript, but it would be better for our purposes to explore Python \u2013 a common language used in back-end programs and scripts \u2013 and its boto3 library for AWS. Here is a good, free Python tutorial.</p> <p>[ ] Tests You should also include some tests for your Python code. Here are some resources on writing good Python tests.</p> <p>[ ] Infrastructure as Code You should not be configuring your API resources \u2013 the DynamoDB table, the API Gateway, the Lambda function \u2013 manually, by clicking around in the AWS console. Instead, define them in an AWS Serverless Application Model (SAM) template and deploy them using the AWS SAM CLI. This is called \u201cinfrastructure as code\u201d or IaC. It saves you time in the long run.</p> <p>Note: A more broadly applicable and commonly-used IaC tool in the industry is Terraform. It\u2019s a little less straightforward to use than SAM for an AWS serverless API, but many people prefer to use it for their project anyway. If you want to use Terraform instead of SAM, follow this guide.</p> <p>[ ] Source Control You do not want to be updating either your back-end API or your front-end website by making calls from your laptop, though. You want them to update automatically whenever you make a change to the code. (This is called continuous integration and deployment, or CI/CD.) Create a GitHub repository for your backend code.</p> <p>[ ] CI/CD (Back end) Set up GitHub Actions such that when you push an update to your Serverless Application Model template or Python code, your Python tests get run. If the tests pass, the SAM application should get packaged and deployed to AWS.</p> <p>[ ] CI/CD (Front end) Create a second GitHub repository for your website code. Create GitHub Actions such that when you push new website code, the S3 bucket automatically gets updated. (You may need to invalidate your CloudFront cache in the code as well.) Important note: DO NOT commit AWS credentials to source control! Bad hats will find them and use them against you!</p> <p>[ ] Blog post Finally, in the text of your resume, you should link a short blog post describing some things you learned while working on this project. Dev.to or Hashnode are great places to publish if you don\u2019t have your own blog.</p> <p>And that\u2019s the gist of it! For strategies, tools, and further challenges to help you get hired in cloud, check out the AWS edition of the Cloud Resume Challenge book.</p>"},{"location":"devops/devops_tools/","title":"Devops tools","text":"<ul> <li>DevOps Tools</li> <li>PowerShell Alias</li> </ul>"},{"location":"devops/devops_tools/#devops-tools","title":"DevOps Tools","text":"<p>This section will cover all of the individual tools utilized in DevOps to deploy HA applications. Some examples will be for Commercial, FedRamp'd, or multi-cloud environments. I'll do my best to let you know when and what. </p>"},{"location":"devops/devops_tools/#powershell-alias","title":"PowerShell Alias","text":"<p>Set an alias in PowerShell <code>set-alias -Name &lt;short_name&gt; -Value &lt;original_name&gt;</code> Real world sample | terraform alias <code>set-alias -Name t -Value terraform</code></p>"},{"location":"gitlab/gitlab/","title":"GitLab","text":""},{"location":"gitlab/gitlab/#transfer-project-into-another-project","title":"Transfer Project into another Project","text":"<p>Open your repository, navigate to <code>General Settings</code>, Expand <code>Advanced</code> and in the <code>Transfer project</code> section, choose a new namespace. </p>"},{"location":"hardware/compute_blade/compute_blade/","title":"Compute Blade","text":"<p>Info</p> <p>Content coming soon. Still migrating documentation. </p> <p>Kickstarter Status: </p>"},{"location":"hardware/raspberry_pi/raspberry_pi/","title":"Raspberry Pi","text":"<p>Info</p> <p>Content coming soon. Still migrating documentation. </p>"},{"location":"hardware/raspberry_pi/raspberry_pi/#3-b","title":"3 B +","text":""},{"location":"hardware/raspberry_pi/raspberry_pi/#4","title":"4","text":""},{"location":"hardware/sparkfun/micromod/","title":"MicroMod","text":"<p>Info</p> <p>Content coming soon. Still migrating documentation. </p>"},{"location":"hardware/sparkfun/micromod/#esp32","title":"ESP32","text":""},{"location":"hardware/sparkfun/micromod/#dual-carrier-boards","title":"Dual Carrier Boards","text":""},{"location":"notion/notion-ai/","title":"Notion-AI","text":"<p>Info</p> <p>Content coming soon. Still migrating documentation. </p>"},{"location":"recovery/backup_recovery/","title":"Backup recovery","text":"<ul> <li>Importance</li> <li>Backups</li> <li>Synology</li> <li>Recovery</li> </ul>"},{"location":"recovery/backup_recovery/#importance","title":"Importance","text":"<p>Recovery - because backups aren't enough. Test your backups! </p>"},{"location":"recovery/backup_recovery/#backups","title":"Backups","text":""},{"location":"recovery/backup_recovery/#synology","title":"Synology","text":"<p>Using Hyper Backup by Synology in the app store, you can easily backup your local files (file share) to an AWS S3 bucket for pennies! Seriously, $1/month per 200GB transferred TO S3 with over 30 days of versions in my initial trial run. </p>"},{"location":"recovery/backup_recovery/#recovery","title":"Recovery","text":""},{"location":"terraform/main%28tf%29/","title":"Terraform","text":"<ul> <li>Terraform</li> <li>Terraform for AWS</li> <li>Structure</li> <li>Main.tf</li> <li>Variables.tf</li> <li>Outputs.tf</li> <li>Alias</li> <li>Random Gotchas</li> <li>Move State (aka., for a specific resource)</li> </ul> <p>Terraform offers a lot of great resources on their website which covers installation, building and destroying infrastructure. </p> <p>Below, you'll find resources that helped me get started or common gotchas along the way. </p>"},{"location":"terraform/main%28tf%29/#terraform-for-aws","title":"Terraform for AWS","text":"<p>Here's the direct link to the AWS Provider documentation. </p>"},{"location":"terraform/main%28tf%29/#structure","title":"Structure","text":""},{"location":"terraform/main%28tf%29/#maintf","title":"Main.tf","text":"<p>This is where you will declare. </p>"},{"location":"terraform/main%28tf%29/#variablestf","title":"Variables.tf","text":"<p>Input variables... it should be <code>input.tf</code> </p>"},{"location":"terraform/main%28tf%29/#outputstf","title":"Outputs.tf","text":"<p>Output variables, that will be spit into the terminal after <code>terraform apply</code></p>"},{"location":"terraform/main%28tf%29/#alias","title":"Alias","text":"<p><code>terraform plan | apply | destroy</code> = <code>tf p | a | d</code></p>"},{"location":"terraform/main%28tf%29/#random-gotchas","title":"Random Gotchas","text":""},{"location":"terraform/main%28tf%29/#move-state-aka-for-a-specific-resource","title":"Move State (aka., for a specific resource)","text":"<p>Today, I decided to refactor my folder structure by deployment instead of 'service'. So originally, I had all of my S3 service configured in <code>/modues/s3/</code>, but now I wanted them to be in <code>/modules/whateverdeployment/</code>. I rebuilt everything thing and hit the <code>terraform plan</code> with all of my buckets flagged for delete!  Terraform doesn't know that it changed because programmatically, it's listed as <code>module.s3.aws_s3_bucket.my_bucket_name</code> and the new resource is <code>module.my_deployment.aws_s3_bucket.my_bucket_name</code>... did you see that \"s3\" is now \"my_deployment\". That's what we are fixing here. </p> <p>Use the move command to migrate the state to the new module. </p> <pre><code>terraform state mv module.s3.aws_s3_bucket.my_bucketname module.my_deployment.aws_s3_bucket.my_bucket\n</code></pre>"}]}